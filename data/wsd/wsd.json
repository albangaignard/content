{
    "additionDate": "2020-01-14T20:11:51Z",
    "biotoolsCURIE": "biotools:WSD",
    "biotoolsID": "WSD",
    "confidence_flag": "low",
    "credit": [
        {
            "email": "zhe.he@cci.fsu.edu",
            "name": "Zhe He",
            "typeEntity": "Person"
        }
    ],
    "description": "Biomedical word sense disambiguation with bidirectional long short-term memory and attention-based neural networks.\n\nWord Sense Disambiguation (WSD) Test Collection.\n\nWord Sense Disambiguation (WSD)Test Collection.\n\nCollaborations & Outside Resources.\n\nEvaluation of Word Sense Disambiguation methods (WSD) in the biomedical domain is difficult because the available resources are either too small or too focused on specific types of entities (e.g. diseases or genes). We have developed a method that can be used to automatically develop a WSD test collection using the Unified Medical Language System (UMLS) Metathesaurus and the manual MeSH indexing of MEDLINE.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/ncbi_resources (NLM.NIH.GOV), bio.tools/genbank (NLM.NIH.GOV).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'BiLSTM'",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Data retrieval",
                    "uri": "http://edamontology.org/operation_2422"
                },
                {
                    "term": "Parsing",
                    "uri": "http://edamontology.org/operation_1812"
                },
                {
                    "term": "Formatting",
                    "uri": "http://edamontology.org/operation_0335"
                }
            ]
        }
    ],
    "homepage": "https://wsd.nlm.nih.gov/collaboration.shtml",
    "lastUpdate": "2020-01-14T20:11:51Z",
    "name": "WSD",
    "owner": "Pub2Tools",
    "publication": [
        {
            "doi": "10.1186/S12859-019-3079-8",
            "metadata": {
                "abstract": "\u00a9 2019 The Author(s).Background: In recent years, deep learning methods have been applied to many natural language processing tasks to achieve state-of-the-art performance. However, in the biomedical domain, they have not out-performed supervised word sense disambiguation (WSD) methods based on support vector machines or random forests, possibly due to inherent similarities of medical word senses. Results: In this paper, we propose two deep-learning-based models for supervised WSD: a model based on bi-directional long short-term memory (BiLSTM) network, and an attention model based on self-attention architecture. Our result shows that the BiLSTM neural network model with a suitable upper layer structure performs even better than the existing state-of-the-art models on the MSH WSD dataset, while our attention model was 3 or 4 times faster than our BiLSTM model with good accuracy. In addition, we trained \"universal\" models in order to disambiguate all ambiguous words together. That is, we concatenate the embedding of the target ambiguous word to the max-pooled vector in the universal models, acting as a \"hint\". The result shows that our universal BiLSTM neural network model yielded about 90 percent accuracy. Conclusion: Deep contextual models based on sequential information processing methods are able to capture the relative contextual information from pre-trained input word embeddings, in order to provide state-of-the-art results for supervised biomedical WSD tasks.",
                "authors": [
                    {
                        "name": "Zhang C."
                    },
                    {
                        "name": "Bis D."
                    },
                    {
                        "name": "Liu X."
                    },
                    {
                        "name": "He Z."
                    }
                ],
                "date": "2019-12-02T00:00:00Z",
                "journal": "BMC Bioinformatics",
                "title": "Biomedical word sense disambiguation with bidirectional long short-term memory and attention-based neural networks"
            },
            "pmcid": "PMC6886160",
            "pmid": "31787096"
        }
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Natural language processing",
            "uri": "http://edamontology.org/topic_0218"
        },
        {
            "term": "Ontology and terminology",
            "uri": "http://edamontology.org/topic_0089"
        },
        {
            "term": "Medicine",
            "uri": "http://edamontology.org/topic_3303"
        }
    ]
}